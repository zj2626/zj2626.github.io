---
title: Python 爬虫实战（1）

comments: true    

tags: 
    - python
    - 爬虫

categories: 
    - 爬虫

description: 只有自己写下了讲出来才能代表真的学会了这么技术

toc: true
   
---

## 前言：

在我学习完python的基础知识之后，当然想要练练手，加深一下对python以及其语法的理解，
所以听说爬虫特别有成就感，非常有利于学习and娱乐，以及培养学习的兴趣，so就到处百度爬虫的相关文章，网上的确有很多相关的，但我还是决定自己写写,
**只有自己写下了讲出来才能代表真的学会了这么技术**

<!--more-->

* 我也是第一次学python和抓包，是根据网上的各种讲解以及自己的摸索，慢慢学会的，有什么说的不对的，欢迎指正 
* 刚开始的时候我的使用Anaconda管理包和环境（py3.6），然而后来我在学多线程的时候，就出现了问题：setdaemon(true)一直没效果（设置守护线程后，守护线程本来应该在所有非守护线程执行完就立马结束而不管守护线程是否结束的，but没用，网上各种查也查不到，后来我把代码写到.py文件里直接在cmd里执行该脚本就没问题了）
* 使用Anaconda是因为我打算入坑深度学习，所以提前熟悉熟悉这个管理工具，科科

### 本章目的： 抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来

+ 豆瓣正在上映列表如图

{% qnimg 20171222001.png title:图片标题 alt:图片说明 extend:?imageView2/2/w/600 %}

+ 豆瓣电影《芳华》短评列表如图

{% qnimg 20171222164027.png title:图片标题 alt:图片说明 extend:?imageView2/2/w/600 %}

+ 最终获得的《芳华》短评词云如图

{% qnimg 20171222170041.png title:图片标题 alt:图片说明 extend:?imageView2/2/w/600 %}

## 个人完整代码：

```python
from urllib import request   #python3.X写法
#import urllib             #python2.X写法
from bs4 import BeautifulSoup as bs
import re       #正则表达式
import jieba    #分词包 中文分词操作 结巴分词
import pandas as pd 
import numpy    #numpy计算包


"""
python2.X 关于 urllib的用法
    import urllib  
    text = urllib.urlopen(url).read()  
    
python3.X 关于 urllib的用法
    import urllib.request  #from urllib import request
    response = urllib.request.urlopen(url)  
    text = response.read()  
"""

def getList():
    resp = request.urlopen('https://movie.douban.com/nowplaying/beijing/')  #获取url下的影片列表;python2.x下使用urllib.urlopen()
    html_data = resp.read().decode('utf-8') # 读取返回的数据(返回页面的html代码)
    # print(html_data)

    soap = bs(html_data, 'html.parser') # 解析html代码 开始获取其中的数据
    nowplaying_movie = soap.find_all('div', id = 'nowplaying')  #获取id为nowplaying的div标签以及内部的代码 (得到的是一个list)
    # print (nowplaying_movie);
    nowplaying_movie_list = nowplaying_movie[0].find_all('li', class_ = 'list-item') #获取class是list-item的所有li标签
    # print (nowplaying_movie_list);
    # print (nowplaying_movie_list[0]['id'], '\n');   # 打印第一个影片的id

    """测试代码 开始"""
    # test = nowplaying_movie_list[0].find_all('ul')
    # print (test)
    # test = nowplaying_movie_list[0].find_all('ul')[0].find_all('li')[1]
    # print (test)
    """测试代码 结束"""
    
    return nowplaying_movie_list

def getComments(nowplaying_movie_list, num):
    requrl = "https://movie.douban.com/subject/" + nowplaying_movie_list[num]['id'] + "/comments?start=0&limit=20" #获取url下的影片短评列表
    resp = request.urlopen(requrl)
    html_data = resp.read().decode('utf-8')

    soap = bs(html_data, 'html.parser')
    
    title = soap.find('title')
    print(title.string)
    
    comment_div_list = soap.find_all('div', class_ = 'comment')
    #print (comment_div_list)
    commentList = []  #存放所有的短评内容数据
    for cm in comment_div_list:
        if cm.find_all('p')[0] is not None:
            commentList.append(cm.find_all('p')[0].string) #把短评内容存放在列表中
    # print (comments)

    comments = ''
    for k in range(len(commentList)):
        comments = comments + (str(commentList[k])).strip()
    #print (comments)
    pattern = re.compile(r'[\u4e00-\u9fa5]+')  #去除标点符号(正则表达式)
    filterdata = re.findall(pattern, comments)
    cleaned_comments = ''.join(filterdata) # 把filterdata按照空字符串为间隔连接起来
    # print (cleaned_comments)

    segment = jieba.lcut(cleaned_comments) #list
    # print (segment)
    words_df = pd.DataFrame({'segment':segment})  #格式转换
    # words_df.head()
    # print(words_df)
    # print (words_df.segment)
    # 数据中有“的”、“是”、“我”、“你”等虚词（停用词），而这些词在任何场景中都是高频时，并且没有实际的含义，所以我们要他们进行清除。

    #从网上下载常用停用词文件 stopwords.txt 然后对比去除统计结果中所有的停用词
    stopwords=pd.read_csv("E:/stopwords.txt",index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')#quoting=3全不引用
    # print (stopwords.stopword)
    # print (words_df.segment.isin(stopwords.stopword))
    words_df = words_df[~words_df.segment.isin(stopwords.stopword)]  #stopwords.txt不能有空格
    words_df.head()

    #进行词频统计
    words_stat = words_df.groupby(by=['segment'])['segment'].agg({"计数":numpy.size}) # 按照segment分类
    words_stat = words_stat.reset_index().sort_values(by=["计数"],ascending=False)  #词频按照 计数 由大到小排列
    words_stat.head()
    
    return words_stat

#词云展示
def show(words_stat):
    import matplotlib.pyplot as plt
    %matplotlib inline

    import matplotlib
    matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)
    from wordcloud import WordCloud #词云包

    wordcloud=WordCloud(font_path="E:/simhei.ttf",background_color="white",max_font_size=80)  #指定字体类型、字体大小和字体颜色
    # print (wordcloud)
    word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}
    # print (word_frequence)

    wordcloud=wordcloud.fit_words(word_frequence)
    plt.imshow(wordcloud)

num = 3 #从0开始, 获取豆瓣最新上映电影短评关键信息
movie_list = getList()
words_stat = getComments(movie_list, num)
show(words_stat)
```

## 别人家的代码【滑稽】：

```python
#coding:utf-8
__author__ = 'hang'
 
import warnings
warnings.filterwarnings("ignore")
import jieba    #分词包
import numpy    #numpy计算包
import codecs   #codecs提供的open方法来指定打开的文件的语言编码，它会在读取的时候自动转换为内部unicode 
import re
import pandas as pd  
import matplotlib.pyplot as plt
from urllib import request
from bs4 import BeautifulSoup as bs
%matplotlib inline
 
import matplotlib
matplotlib.rcParams['figure.figsize'] = (10.0, 5.0)
from wordcloud import WordCloud#词云包
 
#分析网页函数
def getNowPlayingMovie_list():   
    resp = request.urlopen('https://movie.douban.com/nowplaying/hangzhou/')        
    html_data = resp.read().decode('utf-8')    
    soup = bs(html_data, 'html.parser')    
    nowplaying_movie = soup.find_all('div', id='nowplaying')        
    nowplaying_movie_list = nowplaying_movie[0].find_all('li', class_='list-item')    
    nowplaying_list = []    
    for item in nowplaying_movie_list:        
        nowplaying_dict = {}        
        nowplaying_dict['id'] = item['data-subject']       
        for tag_img_item in item.find_all('img'):            
            nowplaying_dict['name'] = tag_img_item['alt']            
            nowplaying_list.append(nowplaying_dict)    
    return nowplaying_list
 
#爬取评论函数
def getCommentsById(movieId, pageNum): 
    eachCommentList = []; 
    if pageNum>0: 
         start = (pageNum-1) * 20 
    else: 
        return False 
    requrl = 'https://movie.douban.com/subject/' + movieId + '/comments' +'?' +'start=' + str(start) + '&limit=20' 
    print(requrl)
    resp = request.urlopen(requrl) 
    html_data = resp.read().decode('utf-8') 
    soup = bs(html_data, 'html.parser') 
    comment_div_lits = soup.find_all('div', class_='comment') 
    for item in comment_div_lits: 
        if item.find_all('p')[0].string is not None:     
            eachCommentList.append(item.find_all('p')[0].string)
    return eachCommentList
 
def main():
    #循环获取第一个电影的前10页评论
    commentList = []
    NowPlayingMovie_list = getNowPlayingMovie_list()
    for i in range(10):    
        num = i + 1 
        commentList_temp = getCommentsById(NowPlayingMovie_list[0]['id'], num)
        commentList.append(commentList_temp)
 
    #将列表中的数据转换为字符串
    comments = ''
    for k in range(len(commentList)):
        comments = comments + (str(commentList[k])).strip()
 
    #使用正则表达式去除标点符号
    pattern = re.compile(r'[\u4e00-\u9fa5]+')
    filterdata = re.findall(pattern, comments)
    cleaned_comments = ''.join(filterdata)
 
    #使用结巴分词进行中文分词
    segment = jieba.lcut(cleaned_comments)
    words_df=pd.DataFrame({'segment':segment})
 
    #去掉停用词
    stopwords=pd.read_csv("stopwords.txt",index_col=False,quoting=3,sep="\t",names=['stopword'], encoding='utf-8')#quoting=3全不引用
    words_df=words_df[~words_df.segment.isin(stopwords.stopword)]
 
    #统计词频
    words_stat=words_df.groupby(by=['segment'])['segment'].agg({"计数":numpy.size})
    words_stat=words_stat.reset_index().sort_values(by=["计数"],ascending=False)
 
    #用词云进行显示
    wordcloud=WordCloud(font_path="simhei.ttf",background_color="white",max_font_size=80)
    word_frequence = {x[0]:x[1] for x in words_stat.head(1000).values}
 
    word_frequence_list = []
    for key in word_frequence:
        temp = (key,word_frequence[key])
        word_frequence_list.append(temp)
 
    wordcloud=wordcloud.fit_words(word_frequence_list)
    plt.imshow(wordcloud)
 
#主函数
main()
```

> 转载自 链接地址: http://python.jobbole.com/88325/?utm_source=blog.jobbole.com&utm_medium=relatedPosts

> 个人博客 欢迎来访： http://blog.zj2626.com