---
title:  使用多种算法对泰坦尼克号乘客获救原因进行分析

comments: true    

tags: 
    - python
    - 机器学习

categories: 
    - 机器学习

description: 使用多种算法对泰坦尼克号乘客获救原因进行分析

toc: true
   
---

```python
import pandas
titanic = pandas.read_csv('titanic_train.csv')
titanic.head(20)
```

```python
# 数据简单统计
titanic.describe()
```

```python
# 由表Age列有部分缺失，影响最终模型效果
# 解决方法： 填充中位数 (使用fillna()方法填充缺失值NaN， 使用median()获得中位数/中值)
titanic['Age'] = titanic['Age'].fillna(titanic['Age'].median())
titanic.describe()
```

```python
# 处理原始数据，其中有的是字符量需要映射为数值
print (titanic['Sex'].unique())
titanic.loc[titanic['Sex'] == 'male', 'Sex'] = 0
titanic.loc[titanic['Sex'] == 'female', 'Sex'] = 1

print (titanic['Embarked'].unique())
titanic['Embarked'] = titanic['Embarked'].fillna('S')# 缺失值填充，填充数量最多的类别
titanic.loc[titanic['Embarked'] == 'S', 'Embarked'] = 0
titanic.loc[titanic['Embarked'] == 'C', 'Embarked'] = 1
titanic.loc[titanic['Embarked'] == 'Q', 'Embarked'] = 2

titanic.head()
```

### 线性回归 步骤

1. 观察数据，填充缺失值，改变数据形式（数据映射）
2. 使用交叉验证减少过拟合风险
3. 使用线性回归算法计算预测值
4. 确定阈值，计算精度

```python
# 线性回归
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import KFold

# 要使用的特征 船仓等级、性别、年龄、兄弟姐妹人数、老人孩子人数、船票价格、上船位置
predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']

# 构建线性回归
alg = LinearRegression()
kf = KFold(titanic.shape[0], n_folds=3, random_state=0)

predictions0 = []
for train, test in kf:
    # print (train.shape, test.shape)
    train_predictors = titanic.loc[train, predictors]
    train_targets = titanic.loc[train, ['Survived']]
    print (train_predictors.shape, train_targets.shape)
    test_features = titanic.loc[test, predictors]
    
    # 训练 拟合数据 使用交叉验证拆分出来的训练集
    alg.fit(train_predictors, train_targets)
    # 验证 使用交叉验证拆分出来的验证集
    test_predictions = alg.predict(test_features)
    
    predictions0.append(test_predictions)

# 三组预测值
print (len(predictions0), '\n\n')

import numpy as np
# 设定一个阈值 大于阈值表示获救 小于阈值表示未获救
predictions = np.concatenate(predictions0, axis=0)

predictions[predictions >  0.5] = 1
predictions[predictions <= 0.5] = 0

print (type(predictions), type(titanic['Survived'].values))
print (predictions.shape,titanic['Survived'].values.reshape(-1, 1).shape)

accuracy = len(predictions[predictions == titanic['Survived'].values.reshape(-1, 1)]) / len(predictions)
print (accuracy)
```

```python
# 随机森林
from sklearn import cross_validation
from sklearn.ensemble import RandomForestClassifier

predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
```

```python
# 构造随机森林
# n_estimators：构造的树个数  min_samples_split：最小切分点  min_samples_leaf：叶子节点最小个数
alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)
# 交叉验证
kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)
# 准确率
scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=kf)
# 平均准确率
print (scores.mean())
```

```python
# 随机森林调优： 修改参数 
'''
> RandomForestClassifier参数：
1. max_features:随机森林允许单个决策树使用特征的最大数量, 增加max_features一般能提高模型的性能,同时降低算法的速度
2. n_estimators:在利用最大投票数或平均值来预测之前，想要建立子树的数量。较多的子树可以让模型有更好的性能，但同时让你的代码变慢
3. min_sample_leaf:最小样本叶片大小
'''
#1 构建随机森林模型(分类器)
alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=10)
#2 构建交叉验证
kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)

'''
> cross_validation.cross_val_score参数：
1. alg:分类器，可以是任何的分类器，比如支持向量机分类器。alg = svm.SVC(kernel='linear', C=1)
2. cv：交叉验证（cross validation）方法，如果cv是一个int数字的话，并且如果提供了raw target参数，那么就代表使用StratifiedKFold分类方式，如果没有提供raw target参数，那么就代表使用KFold分类方式。
3. raw data,raw target：验证使用的数据（feature以及label）

4. 返回值：对于每次不同的的划分raw data时，在test data（验证集）上得到的分类的准确率
'''
#3 进行交叉验证
scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic['Survived'], cv=kf)
print (len(scores), '-----', scores.mean(), '-----', scores)
```

```python
# 随机森林调优： 添加特征
titanic['FamilySize'] = titanic['SibSp'] + titanic['Parch']
titanic['NameLength'] = titanic['Name'].apply(lambda x: len(x))
```

```python
# 特征重要程度
import numpy as np
from sklearn.feature_selection import SelectKBest,f_classif
import matplotlib.pyplot as plt
predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'NameLength']
```

### Univariate feature selection：单变量的特征选择
单变量特征选择的原理是分别单独的计算每个变量的某个统计指标，根据该指标来判断哪些指标重要。剔除那些不重要的指标。

sklearn.feature_selection模块中主要有以下几个方法：

> SelectKBest和SelectPercentile比较相似，前者选择排名排在前n个的变量，后者选择排名排在前n%的变量。而他们通过什么指标来给变量排名呢？这需要二外的指定。

> 对于regression问题，可以使用f_regression指标。对于classification问题，可以使用chi2或者f_classif变量。

*使用的例子：*
```python
from sklearn.feature_selectionimport SelectPercentile, f_classif
selector =SelectPercentile(f_classif, percentile=10)
```

### Recursive feature elimination：循环特征选择
不单独的检验某个变量的价值，而是将其聚集在一起检验。它的基本思想是，对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1（包含空集）。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validationerror。选择error最小的那个子集作为所挑选的特征。

由以下两个方法实现：sklearn.feature_selection.RFE，sklearn.feature_selection.RFECV

### L1-based featureselection：
该思路的原理是：在linearregression模型中，有的时候会得到sparsesolution。意思是说很多变量前面的系数都等于0或者接近于0。这说明这些变量不重要，那么可以将这些变量去除。

### Tree-based featureselection：决策树特征选择
基于决策树算法做出特征选择

```python
selector = SelectKBest(f_classif, k=5)
selector.fit(titanic[predictors], titanic['Survived'])

scores = -np.log10(selector.pvalues_)
print (scores)

plt.bar(range(len(predictors)), scores)
plt.xticks(range(len(predictors)), predictors, rotation='vertical')
plt.show()
```

```python
# 多种算法集成预测
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import numpy as np

# 两个算法集成到一起使用
algorithms = [
    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3),  ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize']],
     [LogisticRegression(random_state=1), ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]
]

kf = KFold(titanic.shape[0], n_folds=3, random_state=1)

predictions = []

# 两个算法分别预测
for train, test in kf:
    train_target = titanic.loc[train, ['Survived']]
    full_test_predictions = []
    
    # 循环分类算法，获得不同算法求得的概率
    for alg, predictors in algorithms:
        # alg：分类算法   predictors：特征
        
        # 使用训练集 拟合数据
        alg.fit(titanic.loc[train, predictors], train_target)
        # 使用验证集 计算 (这里 直接取预测的概率的第二个值-->[:, 1])
        test_predictions = alg.predict_proba(titanic.loc[test, predictors].astype(float))[:, 1]
        full_test_predictions.append(test_predictions)
    
    # 使用不同的分类算法之后计算平均概率
    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2
    test_predictions[test_predictions <= 0.5] = 0
    test_predictions[test_predictions >  0.5] = 1
    
    predictions.append(test_predictions)

# 获得的分类结果
predictions = np.concatenate(predictions, axis=0)

# 计算精确度
accuracy = len(predictions[predictions == titanic['Survived']]) / len(predictions)
print (accuracy)
```

> 个人博客 欢迎来访： http://zj2626.github.io