<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="欢迎来到晋的博客,本博客用来收录平时学习笔记,欢迎访问"><title>Python 爬虫实战（1） | ZJ_BLOG</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.1"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/7.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="/js/instantclick.min.js"></script><script>InstantClick.init();
InstantClick.on('change', function (isInitialLoad) {
         if (isInitialLoad === false) {
         if (typeof MathJax !== 'undefined') // support MathJax
         MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
         if (typeof prettyPrint !== 'undefined') // support google code prettify
         prettyPrint();
         if (typeof _hmt !== 'undefined')  // support 百度统计
         _hmt.push(['_trackPageview', location.pathname + location.search]);
         if (typeof ga !== 'undefined')  // support google analytics
         ga('send', 'pageview', location.pathname + location.search);
         }
 });
</script><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Python 爬虫实战（1）</h1><a id="logo" href="/.">ZJ_BLOG</a><p class="description">Silence的博客</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Python 爬虫实战（1）</h1><div class="post-meta">Jan 15, 2020<span> | </span><span class="category"><a href="/categories/爬虫/">爬虫</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#前言："><span class="toc-number">1.</span> <span class="toc-text">前言：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#本章目的：-抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来"><span class="toc-number">1.1.</span> <span class="toc-text">本章目的： 抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#抓取步骤大致分为三步，具体的又分为下面几步："><span class="toc-number">2.</span> <span class="toc-text">抓取步骤大致分为三步，具体的又分为下面几步：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-访问并获取网页数据并抽取出来有用信息"><span class="toc-number">2.1.</span> <span class="toc-text">1.访问并获取网页数据并抽取出来有用信息</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#首先要获取网页数据"><span class="toc-number">2.1.1.</span> <span class="toc-text">首先要获取网页数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#其次-分析网页数据，抓取想要的数据"><span class="toc-number">2.1.2.</span> <span class="toc-text">其次,分析网页数据，抓取想要的数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-分析网页中有用信息并进行处理"><span class="toc-number">2.2.</span> <span class="toc-text">2.分析网页中有用信息并进行处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中"><span class="toc-number">2.2.1.</span> <span class="toc-text">首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-制作为词云"><span class="toc-number">2.3.</span> <span class="toc-text">3.制作为词云</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#遇到403-forbidden以及503-Service-Unavailable问题的解决方法："><span class="toc-number">3.</span> <span class="toc-text">遇到403: forbidden以及503: Service Unavailable问题的解决方法：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#个人完整代码："><span class="toc-number">4.</span> <span class="toc-text">个人完整代码：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#别人家的代码【滑稽】："><span class="toc-number">5.</span> <span class="toc-text">别人家的代码【滑稽】：</span></a></li></ol></div></div><div class="post-content"><h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><p>在我学习完python的基础知识之后，当然想要练练手，加深一下对python以及其语法的理解，<br>所以听说爬虫特别有成就感，非常有利于学习and娱乐，以及培养学习的兴趣，so就到处百度爬虫的相关文章，网上的确有很多相关的，但我还是决定自己写写,<br><strong>只有自己写下了讲出来才能代表真的学会了这么技术</strong></p>
<a id="more"></a>
<ul>
<li>我也是第一次学python和抓包，是根据网上的各种讲解以及自己的摸索，慢慢学会的，有什么说的不对的，欢迎指正</li>
<li>刚开始的时候我的使用Anaconda管理包和环境（py3.6），然而后来我在学多线程的时候，就出现了问题：setdaemon(true)一直没效果（设置守护线程后，守护线程本来应该在所有非守护线程执行完就立马结束而不管守护线程是否结束的，but没用，网上各种查也查不到，后来我把代码写到.py文件里直接在cmd里执行该脚本就没问题了）</li>
<li>使用Anaconda是因为我打算入坑深度学习，所以提前熟悉熟悉这个管理工具，科科</li>
</ul>
<h3 id="本章目的：-抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来"><a href="#本章目的：-抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来" class="headerlink" title="本章目的： 抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来"></a>本章目的： 抓取豆瓣电影网站正在上映列表的评价关键词，并使用词云表示出来</h3><ul>
<li>豆瓣正在上映列表如图</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171222001.png" title="正在上映">
<ul>
<li>豆瓣电影《芳华》短评列表如图</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171222164027.png" title="芳华">
<ul>
<li>最终获得的《芳华》短评词云如图</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171225134154.png" title="芳华">
<h2 id="抓取步骤大致分为三步，具体的又分为下面几步："><a href="#抓取步骤大致分为三步，具体的又分为下面几步：" class="headerlink" title="抓取步骤大致分为三步，具体的又分为下面几步："></a>抓取步骤大致分为三步，具体的又分为下面几步：</h2><h3 id="1-访问并获取网页数据并抽取出来有用信息"><a href="#1-访问并获取网页数据并抽取出来有用信息" class="headerlink" title="1.访问并获取网页数据并抽取出来有用信息"></a>1.访问并获取网页数据并抽取出来有用信息</h3><ul>
<li><h4 id="首先要获取网页数据"><a href="#首先要获取网页数据" class="headerlink" title="首先要获取网页数据"></a>首先要获取网页数据</h4></li>
</ul>
<blockquote>
<p>获取豆瓣正在上映列表的网页数据<br>使用Python的urllib模块: 其提供了一个从指定的URL地址获取网页数据(创建一个表示远程url的类文件对象)，通过该对象可以对其进行分析处理，获取想要的数据</p>
</blockquote>
<ul>
<li>函数原型</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171225094820.png" title="函数原型 alt:函数原型 extend:?imageView2/2/w/600">
<ol>
<li>url: 请求的地址（除了url，其他都可以不填）</li>
<li>data: 访问url时请求的参数</li>
<li>timeout: 超时时间</li>
</ol>
<ul>
<li>python2.X写法：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">urltext = urllib.urlopen(url)</span><br></pre></td></tr></table></figure>
<ul>
<li>python3.X写法：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request  <span class="comment">#from urllib import request</span></span><br><span class="line">urltext = urllib.request.urlopen(url)</span><br></pre></td></tr></table></figure>
<ul>
<li>urlopen()方法返回值类型：</li>
</ul>
<p>如果请求的是http或者https地址，则返回http.client.HTTPResponse对象<br>如果请求的是ftp或者Data URL地址(以及requests explicitly handled by legacyURLopener and FancyURLopener classes &lt;-原谅我没看懂)，则返回urllib.response.addinfourl对象</p>
<ul>
<li>上面的对象包含多个方法可供我们使用</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171225105358.png" title="来自源码request.py extend:?imageView2/2/w/600">
<ol>
<li>geturl() :返回请求的网页地址</li>
<li>info()   :返回一个httplib.HTTPMessage对象，表示远程服务器返回的头信息</li>
<li>getcode():返回HTTP状态码</li>
<li>read() , readline() , readlines() , fileno() , close() 这些方法的使用方式与文件对象完全一样</li>
</ol>
<blockquote>
<p>实例</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">urltext = urllib.request.urlopen(<span class="string">'https://movie.douban.com/nowplaying/beijing/'</span>)</span><br><span class="line"><span class="keyword">print</span> (urltext)</span><br><span class="line"><span class="keyword">print</span> (urltext.geturl())</span><br><span class="line"><span class="keyword">print</span> (urltext.info())</span><br><span class="line"><span class="keyword">print</span> (urltext.getcode())</span><br></pre></td></tr></table></figure>
<img src="/2020/01/15/20171214_crawler/20171225111200.png" title="实例 extend:?imageView2/2/w/600">
<img src="/2020/01/15/20171214_crawler/20171225113314.png" title="实例 extend:?imageView2/2/w/600">
<ul>
<li><h4 id="其次-分析网页数据，抓取想要的数据"><a href="#其次-分析网页数据，抓取想要的数据" class="headerlink" title="其次,分析网页数据，抓取想要的数据"></a>其次,分析网页数据，抓取想要的数据</h4></li>
</ul>
<blockquote>
<p>找到网页上你要的电影列表的位置，看看有什么标签特点</p>
</blockquote>
<img src="/2020/01/15/20171214_crawler/20171225112215.png" title="来自豆瓣 extend:?imageView2/2/w/600">
<p>我们发现所有的电影列表都在id为nowplaying的div下面的一个ul下，该ul的class为lists，并且每个电影的li标签的class为list-item<br>该li标签中有许多熟悉，我们发现data-title为电影标题，data-score为电影评分，data-star为打星…,最最重要的是id，每个电影都不同，可推测应该是电影的唯一标识（编号）；<br>我们要通过某一个标识来查询该电影的短评， 通过查看电影主页的网址（<a href="https://movie.douban.com/subject/26862829/" target="_blank" rel="noopener">https://movie.douban.com/subject/26862829/</a> ）可知，这个id就是我们需要的</p>
<blockquote>
<p>使用python的BeautifulSoup库进行网页信息的抓取（网页解析库）<br>BeautifulSoup 是一个可以从 HTML 或 XML 文件中提取数据的 Python 库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup 会帮你节省数小时甚至数天的工作时间.</p>
</blockquote>
<ul>
<li>BeautifulSoup:Beautiful Soup提供一些简单的、python式的函数用来处理导航、搜索、修改分析树等功能。<br>它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。<br>Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。<br>Beautiful Soup已成为和lxml、html6lib一样出色的python解释器，为用户灵活地提供不同的解析策略或强劲的速度。(from <a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">http://beautifulsoup.readthedocs.io/zh_CN/latest/</a> )</li>
</ul>
<ul>
<li>使用BeautifulSoup解析代码,能够得到一个 BeautifulSoup 的对象,并能按照标准的缩进格式的结构输出</li>
<li>Beautiful Soup将复杂HTML文档转换成一个复杂的树形结构,每个节点都是Python对象,所有对象可以归纳为4种: Tag, NavigableString, BeautifulSoup, Comment<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Tag: &lt;class &apos;bs4.element.Tag&apos;&gt;标签对象，两个属性：name, attribute (直接调用：tag.name，tag[&apos;class&apos;]), 如果是多值属性，则返回list，也可以赋值为多值属性（假的多值属性返回字符串，如id=&quot;aaa bbb&quot;）</span><br><span class="line">NavigableString: &lt;class &apos;bs4.element.NavigableString&apos;&gt; tag中的字符串对象，即tag.string; tag中包含的字符串不能编辑,但是可以被替换成其它的字符串,用 replace_with() 方法</span><br><span class="line">BeautifulSoup : &lt;class &apos;bs4.BeautifulSoup&apos;&gt;BeautifulSoup对象表示的是一个文档的全部内容.大部分时候,可以把它当作 Tag 对象,它支持 遍历文档树 和 搜索文档树 中描述的大部分的方法; 一个方法：soup.name # [document]</span><br><span class="line">Comment : &lt;class &apos;bs4.element.Comment&apos;&gt;文档的注释部分，Comment 对象是一个特殊类型的 NavigableString 对象</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>BeautifulSoup对象使用示例：</p>
<ol>
<li><p>解析时，可以传入一段字符串或一个文件句柄.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup   <span class="comment">#导入模块</span></span><br><span class="line">soup = BeautifulSoup(open(<span class="string">"index.html"</span>))</span><br><span class="line">soup = BeautifulSoup(<span class="string">"&lt;html&gt;data&lt;/html&gt;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#首先,文档被转换成Unicode,并且HTML的实例都被转换成Unicode编码</span></span><br><span class="line"><span class="comment">#然后,BeautifulSoup选择最合适的解析器来解析这段文档,如果手动指定解析器那么Beautiful Soup会选择指定的解析器来解析文档</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>soup.title           # <title>标签对象：<title>北京 - 在线购票&amp;影讯</title></title></p>
</li>
<li>soup.title.name      # <title>标签名称：title</title></li>
<li>soup.title.string    # <title>标签内容：北京 - 在线购票&amp;影讯</title></li>
<li>soup.p               # 第一个p标签对象：<p class="appintro-title">豆瓣</p></li>
<li><p>soup.p[‘class’]      # 第一个p标签对象的类属性<br>7.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">原型：find_all( name , attrs , recursive , string , **kwargs ) 搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件</span><br><span class="line">    name:       name 参数可以查找所有名字为 name 的tag,字符串对象会被自动忽略掉.</span><br><span class="line">    attrs:      通过属性选择器查询，有两种写法</span><br><span class="line">                    1. soup.find_all(class_=&apos;value&apos;, id=&apos;value2&apos;)</span><br><span class="line">                    2. soup.find_all(attrs=&#123;&quot;class&quot;: &quot;value&quot;, &quot;id&quot;:&quot;value2&quot;&#125;)</span><br><span class="line">    limit:  限制查询结果个数</span><br><span class="line">    recursive: 调用tag的 find_all() 方法时,Beautiful Soup会检索当前tag的所有子孙节点,如果只想搜索tag的直接子节点,可以使用参数 recursive=False</span><br><span class="line">    string: 通过 string 参数可以搜搜文档中的字符串内容. soup.find_all(&quot;a&quot;, string=&quot;value&quot;) #查询标签中文字包含value的a标签</span><br></pre></td></tr></table></figure>
</li>
<li><p>soup.find(‘a’).get(‘href’)   # 找到第一个a标签 并返回其href属性内容 （ find_all() 方法的返回结果是值包含一个元素的列表,而 find() 方法直接返回结果.）</p>
</li>
<li>更多用法见BeautifulSoup官网中文文档：<a href="http://beautifulsoup.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">http://beautifulsoup.readthedocs.io/zh_CN/latest/</a></li>
</ol>
<ul>
<li><p>解析网页代码,并编码为utf-8</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">urltext = urllib.request.urlopen(<span class="string">'https://movie.douban.com/nowplaying/beijing/'</span>)</span><br><span class="line">html_data = urltext.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">print</span> (html_data)</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取正在上映列表数据 nowplaying_movie_list列表（List）</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line">urltext = urllib.request.urlopen(<span class="string">'https://movie.douban.com/nowplaying/beijing/'</span>)</span><br><span class="line">html_data = urltext.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line">soup = bs(html_data, <span class="string">'html.parser'</span>)</span><br><span class="line">nowplaying_movie = soup.find_all(<span class="string">'div'</span>, id = <span class="string">'nowplaying'</span>) <span class="comment"># 先获取id为nowplaying的div</span></span><br><span class="line"><span class="comment"># print (nowplaying_movie) # 只有一条数据，因为id是唯一的</span></span><br><span class="line">nowplaying_movie_list = nowplaying_movie[<span class="number">0</span>].find_all(<span class="string">'li'</span>, class_ = <span class="string">'list-item'</span>)<span class="comment"># 再获取class为list-item的li</span></span><br><span class="line"><span class="keyword">print</span> (nowplaying_movie_list)</span><br></pre></td></tr></table></figure>
<ul>
<li>至此已经获得了最内部一层的电影数据， 可以直接获得每个电影的id了</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> (nowplaying_movie_list[<span class="number">0</span>][<span class="string">'id'</span>], <span class="string">'\n'</span>) <span class="comment">#获取第一个电影的id数据</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>现在 我们需要获取其中某一个id，通过这个id获取对应电影的短评，然后就可以进行处理了</p>
</li>
<li><p>你也可以自由发挥，制作一个查询的功能，通过输入电影名称指定某一个电影进行分析</p>
</li>
</ul>
<h3 id="2-分析网页中有用信息并进行处理"><a href="#2-分析网页中有用信息并进行处理" class="headerlink" title="2.分析网页中有用信息并进行处理"></a>2.分析网页中有用信息并进行处理</h3><ul>
<li><h4 id="首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中"><a href="#首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中" class="headerlink" title="首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中"></a>首先按照上面的步骤访问电影首页，抽取短评信息，存放到一个List中</h4></li>
<li><p>首先解析网页代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">requrl = <span class="string">"https://movie.douban.com/subject/"</span> + nowplaying_movie_list[<span class="number">0</span>][<span class="string">'id'</span>] + <span class="string">"/comments?start=0&amp;limit=20"</span></span><br><span class="line">resp = urllib.request.urlopen(requrl)</span><br><span class="line">html_data = resp.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">soup = bs(html_data, <span class="string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">title = soup.find(<span class="string">'title'</span>) <span class="comment"># 直接获取title标签</span></span><br><span class="line">print(title.string) <span class="comment">#获取标签中内容</span></span><br><span class="line">comment_div_list = soup.find_all(<span class="string">'div'</span>, class_ = <span class="string">'comment'</span>)</span><br><span class="line"><span class="keyword">print</span> (comment_div_list) <span class="comment">#所有的短片标签列表</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>通过下面的源码可知，所有的短评文字都放在class为comment-item的div下的一个p标签中，所有我们要得到所有的p标签并组成一个List</p>
</li>
</ul>
<img src="/2020/01/15/20171214_crawler/20171225120114.png" title="来自豆瓣 extend:?imageView2/2/w/600">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">commentList = []  <span class="comment">#存放所有的短评内容数据 List</span></span><br><span class="line"><span class="keyword">for</span> cm <span class="keyword">in</span> comment_div_list:</span><br><span class="line">    <span class="keyword">if</span> cm.find_all(<span class="string">'p'</span>)[<span class="number">0</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        commentList.append(cm.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].string) <span class="comment">#把短评内容存放在列表中</span></span><br><span class="line"><span class="keyword">print</span> (commentList)</span><br></pre></td></tr></table></figure>
<ul>
<li>已得短评List，但是该List中包含大量的单引号（List自带的），换行符等不需要的东西，并且由于我们要做成词云，所有的符号都不要，只要文字</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="literal">None</span> <span class="keyword">in</span> commentList:</span><br><span class="line">        commentList.remove(<span class="literal">None</span>) <span class="comment">#去除NoneType数据</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">comments = <span class="string">''</span>.join(commentList) <span class="comment">#拼接字符串</span></span><br><span class="line">comments = comments.replace(<span class="string">' '</span>,<span class="string">''</span>).replace(<span class="string">"\n"</span>, <span class="string">""</span>).replace(<span class="string">"\t"</span>, <span class="string">""</span>)</span><br><span class="line"><span class="keyword">print</span> (comments)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>词云展示的只是关键词，所以去除用户短评中的所有的标点符号（正则表达式）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re       <span class="comment">#正则表达式</span></span><br><span class="line">pattern = re.compile(<span class="string">r'[\u4e00-\u9fa5]+'</span>)  <span class="comment">#去除标点符号(正则表达式)</span></span><br><span class="line">filterdata = re.findall(pattern, comments)</span><br><span class="line">cleaned_comments = <span class="string">''</span>.join(filterdata) <span class="comment"># 把filterdata按照空字符串为间隔连接起来</span></span><br><span class="line"><span class="keyword">print</span> (cleaned_comments)</span><br></pre></td></tr></table></figure>
</li>
<li><p>目前所有的评价都没有间隔的展示在这里，我们需要把其中的词语取出来得到所有的关键词</p>
<blockquote>
<p>使用jieba分词, 把字符串中的所有的词语分出来，组成一个List</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>结巴（jieba）是国人出的一个精品插件，可以对一段中文进行分词，有三种分词模式，可以适应不同需求。</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">jieba.cut 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型</span><br><span class="line">jieba.cut_for_search 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细</span><br><span class="line">待分词的字符串可以是 unicode 或 UTF-8 字符串、GBK 字符串。注意：不建议直接输入 GBK 字符串，可能无法预料地错误解码成 UTF-8</span><br><span class="line">jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator，可以使用 for 循环来获得分词后得到的每一个词语(unicode)，或者用</span><br><span class="line">jieba.lcut 以及 jieba.lcut_for_search 直接返回 list</span><br><span class="line">jieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。</span><br><span class="line"></span><br><span class="line">也可以添加自定义词典 （from： http://blog.csdn.net/qq_27231343/article/details/51898940 ）</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#代码示例</span></span><br><span class="line"><span class="comment"># encoding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">"Full Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 全模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"我来到北京清华大学"</span>, cut_all=<span class="literal">False</span>)</span><br><span class="line">print(<span class="string">"Default Mode: "</span> + <span class="string">"/ "</span>.join(seg_list))  <span class="comment"># 精确模式</span></span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(<span class="string">"他来到了网易杭研大厦"</span>)  <span class="comment"># 默认是精确模式</span></span><br><span class="line">print(<span class="string">"* "</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut_for_search(<span class="string">"小明硕士毕业于中国科学院计算所，后在日本京都大学深造"</span>)  <span class="comment"># 搜索引擎模式</span></span><br><span class="line">print(<span class="string">", "</span>.join(seg_list))</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出结果</span></span><br><span class="line"><span class="comment"># Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学</span></span><br><span class="line"><span class="comment"># Default Mode: 我/ 来到/ 北京/ 清华大学</span></span><br><span class="line"><span class="comment"># 他* 来到* 了* 网易* 杭研* 大厦       (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)</span></span><br><span class="line"><span class="comment"># 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>使用jieba分割短评，获取返回的分词List</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line">segment = jieba.lcut(cleaned_comments)</span><br><span class="line"><span class="keyword">print</span> (segment)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据中有“的”、“是”、“我”、“你”等虚词（停用词），而这些词在任何场景中都是高频时，并且没有实际的含义，所以我们要他们进行清除。</p>
<blockquote>
<p>使用pandas</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">words_df = pd.DataFrame(&#123;<span class="string">'segment'</span>:segment&#125;)  <span class="comment">#格式转换 把List转化为Dict</span></span><br><span class="line"><span class="comment"># words_df.head()</span></span><br><span class="line"><span class="comment"># print(words_df)</span></span><br><span class="line"><span class="comment"># print (words_df.segment)</span></span><br><span class="line"><span class="comment">#从网上下载常用停用词文件 stopwords.txt 然后对比去除统计结果中所有的停用词</span></span><br><span class="line">stopwords=pd.read_csv(<span class="string">"E:/stopwords.txt"</span>,index_col=<span class="literal">False</span>,quoting=<span class="number">3</span>,sep=<span class="string">"\t"</span>,names=[<span class="string">'stopword'</span>], encoding=<span class="string">'utf-8'</span>)<span class="comment">#quoting=3全不引用</span></span><br><span class="line"><span class="comment"># print (stopwords.stopword)</span></span><br><span class="line"><span class="comment"># print (words_df.segment.isin(stopwords.stopword))</span></span><br><span class="line">words_df = words_df[~words_df.segment.isin(stopwords.stopword)]  <span class="comment">#stopwords.txt不能有空格</span></span><br><span class="line">words_df.head()</span><br></pre></td></tr></table></figure>
<p><strong>我的停用词文件： <a href="http://p18j2ow6f.bkt.clouddn.com/static/file/stopwords.txt" target="_blank" rel="noopener">http://p18j2ow6f.bkt.clouddn.com/static/file/stopwords.txt</a></strong></p>
<ul>
<li>清洗了关键词以后，我们把剩下的词语进行分类统计，观察每个词语的频率<blockquote>
<p>使用numpy</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy    <span class="comment">#numpy计算包</span></span><br><span class="line">words_stat = words_df.groupby(by=[<span class="string">'segment'</span>])[<span class="string">'segment'</span>].agg(&#123;<span class="string">"计数"</span>:numpy.size&#125;) <span class="comment"># 按照segment分类</span></span><br><span class="line">words_stat = words_stat.reset_index().sort_values(by=[<span class="string">"计数"</span>],ascending=<span class="literal">False</span>)  <span class="comment">#词频按照 计数 由大到小排列</span></span><br><span class="line">words_stat.head()</span><br></pre></td></tr></table></figure>
</blockquote>
</li>
</ul>
<h3 id="3-制作为词云"><a href="#3-制作为词云" class="headerlink" title="3.制作为词云"></a>3.制作为词云</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"></span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">5.0</span>)</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud <span class="comment">#词云包</span></span><br><span class="line"></span><br><span class="line">wordcloud=WordCloud(font_path=<span class="string">"E:/simhei.ttf"</span>,background_color=<span class="string">"white"</span>,max_font_size=<span class="number">80</span>)  <span class="comment">#指定字体类型、字体大小和字体颜色</span></span><br><span class="line"><span class="comment"># print (wordcloud)</span></span><br><span class="line">word_frequence = &#123;x[<span class="number">0</span>]:x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> words_stat.head(<span class="number">1000</span>).values&#125;</span><br><span class="line"><span class="comment"># print (word_frequence)</span></span><br><span class="line"></span><br><span class="line">wordcloud=wordcloud.fit_words(word_frequence)</span><br><span class="line">matplotlib.pyplot.imshow(wordcloud)</span><br></pre></td></tr></table></figure>
<p><strong>我的字体文件： <a href="http://p18j2ow6f.bkt.clouddn.com/static/file/simhei.ttf" target="_blank" rel="noopener">http://p18j2ow6f.bkt.clouddn.com/static/file/simhei.ttf</a></strong></p>
<blockquote>
<p>最终效果</p>
</blockquote>
<img src="/2020/01/15/20171214_crawler/20171225134154.png" title="芳华">
<h2 id="遇到403-forbidden以及503-Service-Unavailable问题的解决方法："><a href="#遇到403-forbidden以及503-Service-Unavailable问题的解决方法：" class="headerlink" title="遇到403: forbidden以及503: Service Unavailable问题的解决方法："></a>遇到403: forbidden以及503: Service Unavailable问题的解决方法：</h2><p>这是网站对自动化爬虫的禁止需要用python的模块urllib2模块(对于3.6版本使用 urllib.request)</p>
<p><em>User-Agent是浏览器特有的属性，通过浏览器查看源代码就可以查看到(其他的属性也可以通过浏览器点击F12中的network窗口发现)</em></p>
<img src="/2020/01/15/20171214_crawler/20180101164037.png" title="403问题解决">
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCodes</span><span class="params">()</span>:</span></span><br><span class="line">    headers=[<span class="string">"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"</span>]</span><br><span class="line">    randdom_header=random.choice(headers)</span><br><span class="line"></span><br><span class="line">    url = <span class="string">'http://bj.meituan.com/meishi/c17/'</span>;</span><br><span class="line">    req=request.Request(url)</span><br><span class="line">    req.add_header(<span class="string">"User-Agent"</span>,randdom_header)</span><br><span class="line">    req.add_header(<span class="string">"Host"</span>,<span class="string">"bj.meituan.com"</span>)</span><br><span class="line">    req.add_header(<span class="string">"Referer"</span>,<span class="string">"http://bj.meituan.com/"</span>)</span><br><span class="line">    req.add_header(<span class="string">"GET"</span>,url)</span><br><span class="line">    resp_text=request.urlopen(req).read()</span><br><span class="line"></span><br><span class="line">    soap = bs(resp_text, <span class="string">'html.parser'</span>)</span><br><span class="line">    list = soap.find_all(<span class="string">'ul'</span>, class_ = <span class="string">'list-ul'</span>)[<span class="number">0</span>].find_all(<span class="string">'li'</span>)</span><br><span class="line">    <span class="keyword">print</span> (list)</span><br><span class="line"></span><br><span class="line">    codeList = []</span><br><span class="line">    <span class="keyword">for</span> div <span class="keyword">in</span> list:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            url = div.find(<span class="string">'div'</span>, class_ = <span class="string">'img '</span>).find(<span class="string">'a'</span>)[<span class="string">'href'</span>]</span><br><span class="line"><span class="comment">#             len = url.index('?')</span></span><br><span class="line">            codeList.append(url) <span class="comment"># url[: len]</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> codeList</span><br></pre></td></tr></table></figure>
<h2 id="个人完整代码："><a href="#个人完整代码：" class="headerlink" title="个人完整代码："></a>个人完整代码：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request   <span class="comment">#python3.X写法</span></span><br><span class="line"><span class="comment">#import urllib             #python2.X写法</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line"><span class="keyword">import</span> re       <span class="comment">#正则表达式</span></span><br><span class="line"><span class="keyword">import</span> jieba    <span class="comment">#分词包 中文分词操作 结巴分词</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy    <span class="comment">#numpy计算包</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">python2.X 关于 urllib的用法</span></span><br><span class="line"><span class="string">    import urllib</span></span><br><span class="line"><span class="string">    text = urllib.urlopen(url).read()</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">python3.X 关于 urllib的用法</span></span><br><span class="line"><span class="string">    import urllib.request  #from urllib import request</span></span><br><span class="line"><span class="string">    response = urllib.request.urlopen(url)</span></span><br><span class="line"><span class="string">    text = response.read()</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getList</span><span class="params">()</span>:</span></span><br><span class="line">    resp = request.urlopen(<span class="string">'https://movie.douban.com/nowplaying/beijing/'</span>)  <span class="comment">#获取url下的影片列表;python2.x下使用urllib.urlopen()</span></span><br><span class="line">    html_data = resp.read().decode(<span class="string">'utf-8'</span>) <span class="comment"># 读取返回的数据(返回页面的html代码)</span></span><br><span class="line">    <span class="comment"># print(html_data)</span></span><br><span class="line"></span><br><span class="line">    soup = bs(html_data, <span class="string">'html.parser'</span>) <span class="comment"># 解析html代码 开始获取其中的数据</span></span><br><span class="line">    nowplaying_movie = soup.find_all(<span class="string">'div'</span>, id = <span class="string">'nowplaying'</span>)  <span class="comment">#获取id为nowplaying的div标签以及内部的代码 (得到的是一个list)</span></span><br><span class="line">    <span class="comment"># print (nowplaying_movie);</span></span><br><span class="line">    nowplaying_movie_list = nowplaying_movie[<span class="number">0</span>].find_all(<span class="string">'li'</span>, class_ = <span class="string">'list-item'</span>) <span class="comment">#获取class是list-item的所有li标签</span></span><br><span class="line">    <span class="comment"># print (nowplaying_movie_list);</span></span><br><span class="line">    <span class="comment"># print (nowplaying_movie_list[0]['id'], '\n');   # 打印第一个影片的id</span></span><br><span class="line"></span><br><span class="line">    <span class="string">"""测试代码 开始"""</span></span><br><span class="line">    <span class="comment"># test = nowplaying_movie_list[0].find_all('ul')</span></span><br><span class="line">    <span class="comment"># print (test)</span></span><br><span class="line">    <span class="comment"># test = nowplaying_movie_list[0].find_all('ul')[0].find_all('li')[1]</span></span><br><span class="line">    <span class="comment"># print (test)</span></span><br><span class="line">    <span class="string">"""测试代码 结束"""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nowplaying_movie_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getComments</span><span class="params">(nowplaying_movie_list, num)</span>:</span></span><br><span class="line">    requrl = <span class="string">"https://movie.douban.com/subject/"</span> + nowplaying_movie_list[num][<span class="string">'id'</span>] + <span class="string">"/comments?start=0&amp;limit=20"</span> <span class="comment">#获取url下的影片短评列表</span></span><br><span class="line">    resp = urllib.request.urlopen(requrl)</span><br><span class="line">    html_data = resp.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line"></span><br><span class="line">    soup = bs(html_data, <span class="string">'html.parser'</span>)</span><br><span class="line"></span><br><span class="line">    title = soup.find(<span class="string">'title'</span>)</span><br><span class="line">    print(title.string)</span><br><span class="line"></span><br><span class="line">    comment_div_list = soup.find_all(<span class="string">'div'</span>, class_ = <span class="string">'comment'</span>)</span><br><span class="line">    <span class="comment">#print (comment_div_list)</span></span><br><span class="line">    commentList = []  <span class="comment">#存放所有的短评内容数据</span></span><br><span class="line">    <span class="keyword">for</span> cm <span class="keyword">in</span> comment_div_list:</span><br><span class="line">        <span class="keyword">if</span> cm.find_all(<span class="string">'p'</span>)[<span class="number">0</span>] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            commentList.append(cm.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].string) <span class="comment">#把短评内容存放在列表中</span></span><br><span class="line">    <span class="comment"># print (comments)</span></span><br><span class="line"></span><br><span class="line">    comments = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(commentList)):</span><br><span class="line">        comments = comments + (str(commentList[k])).strip()</span><br><span class="line">    <span class="comment">#print (comments)</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'[\u4e00-\u9fa5]+'</span>)  <span class="comment">#去除标点符号(正则表达式)</span></span><br><span class="line">    filterdata = re.findall(pattern, comments)</span><br><span class="line">    cleaned_comments = <span class="string">''</span>.join(filterdata) <span class="comment"># 把filterdata按照空字符串为间隔连接起来</span></span><br><span class="line">    <span class="comment"># print (cleaned_comments)</span></span><br><span class="line"></span><br><span class="line">    segment = jieba.lcut(cleaned_comments) <span class="comment">#list</span></span><br><span class="line">    <span class="comment"># print (segment)</span></span><br><span class="line">    words_df = pd.DataFrame(&#123;<span class="string">'segment'</span>:segment&#125;)  <span class="comment">#格式转换</span></span><br><span class="line">    <span class="comment"># words_df.head()</span></span><br><span class="line">    <span class="comment"># print(words_df)</span></span><br><span class="line">    <span class="comment"># print (words_df.segment)</span></span><br><span class="line">    <span class="comment"># 数据中有“的”、“是”、“我”、“你”等虚词（停用词），而这些词在任何场景中都是高频时，并且没有实际的含义，所以我们要他们进行清除。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#从网上下载常用停用词文件 stopwords.txt 然后对比去除统计结果中所有的停用词</span></span><br><span class="line">    stopwords=pd.read_csv(<span class="string">"E:/stopwords.txt"</span>,index_col=<span class="literal">False</span>,quoting=<span class="number">3</span>,sep=<span class="string">"\t"</span>,names=[<span class="string">'stopword'</span>], encoding=<span class="string">'utf-8'</span>)<span class="comment">#quoting=3全不引用</span></span><br><span class="line">    <span class="comment"># print (stopwords.stopword)</span></span><br><span class="line">    <span class="comment"># print (words_df.segment.isin(stopwords.stopword))</span></span><br><span class="line">    words_df = words_df[~words_df.segment.isin(stopwords.stopword)]  <span class="comment">#stopwords.txt不能有空格</span></span><br><span class="line">    words_df.head()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#进行词频统计</span></span><br><span class="line">    words_stat = words_df.groupby(by=[<span class="string">'segment'</span>])[<span class="string">'segment'</span>].agg(&#123;<span class="string">"计数"</span>:numpy.size&#125;) <span class="comment"># 按照segment分类</span></span><br><span class="line">    words_stat = words_stat.reset_index().sort_values(by=[<span class="string">"计数"</span>],ascending=<span class="literal">False</span>)  <span class="comment">#词频按照 计数 由大到小排列</span></span><br><span class="line">    words_stat.head()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> words_stat</span><br><span class="line"></span><br><span class="line"><span class="comment">#词云展示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">show</span><span class="params">(words_stat)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib</span><br><span class="line">    %matplotlib inline</span><br><span class="line"></span><br><span class="line">    matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">5.0</span>)</span><br><span class="line">    <span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud <span class="comment">#词云包</span></span><br><span class="line"></span><br><span class="line">    wordcloud=WordCloud(font_path=<span class="string">"E:/simhei.ttf"</span>,background_color=<span class="string">"white"</span>,max_font_size=<span class="number">80</span>)  <span class="comment">#指定字体类型、字体大小和字体颜色</span></span><br><span class="line">    <span class="comment"># print (wordcloud)</span></span><br><span class="line">    word_frequence = &#123;x[<span class="number">0</span>]:x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> words_stat.head(<span class="number">1000</span>).values&#125;</span><br><span class="line">    <span class="comment"># print (word_frequence)</span></span><br><span class="line"></span><br><span class="line">    wordcloud=wordcloud.fit_words(word_frequence)</span><br><span class="line">    matplotlib.pyplot.imshow(wordcloud)</span><br><span class="line"></span><br><span class="line">num = <span class="number">0</span> <span class="comment">#从0开始, 获取豆瓣最新上映电影短评关键信息</span></span><br><span class="line">movie_list = getList()</span><br><span class="line">words_stat = getComments(movie_list, num)</span><br><span class="line">show(words_stat)</span><br></pre></td></tr></table></figure>
<h2 id="别人家的代码【滑稽】："><a href="#别人家的代码【滑稽】：" class="headerlink" title="别人家的代码【滑稽】："></a>别人家的代码【滑稽】：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:utf-8</span></span><br><span class="line">__author__ = <span class="string">'hang'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line"><span class="keyword">import</span> jieba    <span class="comment">#分词包</span></span><br><span class="line"><span class="keyword">import</span> numpy    <span class="comment">#numpy计算包</span></span><br><span class="line"><span class="keyword">import</span> codecs   <span class="comment">#codecs提供的open方法来指定打开的文件的语言编码，它会在读取的时候自动转换为内部unicode</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> bs</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line">matplotlib.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">5.0</span>)</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud<span class="comment">#词云包</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#分析网页函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNowPlayingMovie_list</span><span class="params">()</span>:</span></span><br><span class="line">    resp = request.urlopen(<span class="string">'https://movie.douban.com/nowplaying/hangzhou/'</span>)</span><br><span class="line">    html_data = resp.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    soup = bs(html_data, <span class="string">'html.parser'</span>)</span><br><span class="line">    nowplaying_movie = soup.find_all(<span class="string">'div'</span>, id=<span class="string">'nowplaying'</span>)</span><br><span class="line">    nowplaying_movie_list = nowplaying_movie[<span class="number">0</span>].find_all(<span class="string">'li'</span>, class_=<span class="string">'list-item'</span>)</span><br><span class="line">    nowplaying_list = []</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> nowplaying_movie_list:</span><br><span class="line">        nowplaying_dict = &#123;&#125;</span><br><span class="line">        nowplaying_dict[<span class="string">'id'</span>] = item[<span class="string">'data-subject'</span>]</span><br><span class="line">        <span class="keyword">for</span> tag_img_item <span class="keyword">in</span> item.find_all(<span class="string">'img'</span>):</span><br><span class="line">            nowplaying_dict[<span class="string">'name'</span>] = tag_img_item[<span class="string">'alt'</span>]</span><br><span class="line">            nowplaying_list.append(nowplaying_dict)</span><br><span class="line">    <span class="keyword">return</span> nowplaying_list</span><br><span class="line"></span><br><span class="line"><span class="comment">#爬取评论函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCommentsById</span><span class="params">(movieId, pageNum)</span>:</span></span><br><span class="line">    eachCommentList = [];</span><br><span class="line">    <span class="keyword">if</span> pageNum&gt;<span class="number">0</span>:</span><br><span class="line">         start = (pageNum<span class="number">-1</span>) * <span class="number">20</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    requrl = <span class="string">'https://movie.douban.com/subject/'</span> + movieId + <span class="string">'/comments'</span> +<span class="string">'?'</span> +<span class="string">'start='</span> + str(start) + <span class="string">'&amp;limit=20'</span></span><br><span class="line">    print(requrl)</span><br><span class="line">    resp = request.urlopen(requrl)</span><br><span class="line">    html_data = resp.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    soup = bs(html_data, <span class="string">'html.parser'</span>)</span><br><span class="line">    comment_div_lits = soup.find_all(<span class="string">'div'</span>, class_=<span class="string">'comment'</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> comment_div_lits:</span><br><span class="line">        <span class="keyword">if</span> item.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].string <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            eachCommentList.append(item.find_all(<span class="string">'p'</span>)[<span class="number">0</span>].string)</span><br><span class="line">    <span class="keyword">return</span> eachCommentList</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment">#循环获取第一个电影的前10页评论</span></span><br><span class="line">    commentList = []</span><br><span class="line">    NowPlayingMovie_list = getNowPlayingMovie_list()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">        num = i + <span class="number">1</span></span><br><span class="line">        commentList_temp = getCommentsById(NowPlayingMovie_list[<span class="number">0</span>][<span class="string">'id'</span>], num)</span><br><span class="line">        commentList.append(commentList_temp)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将列表中的数据转换为字符串</span></span><br><span class="line">    comments = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(len(commentList)):</span><br><span class="line">        comments = comments + (str(commentList[k])).strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#使用正则表达式去除标点符号</span></span><br><span class="line">    pattern = re.compile(<span class="string">r'[\u4e00-\u9fa5]+'</span>)</span><br><span class="line">    filterdata = re.findall(pattern, comments)</span><br><span class="line">    cleaned_comments = <span class="string">''</span>.join(filterdata)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#使用结巴分词进行中文分词</span></span><br><span class="line">    segment = jieba.lcut(cleaned_comments)</span><br><span class="line">    words_df=pd.DataFrame(&#123;<span class="string">'segment'</span>:segment&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#去掉停用词</span></span><br><span class="line">    stopwords=pd.read_csv(<span class="string">"stopwords.txt"</span>,index_col=<span class="literal">False</span>,quoting=<span class="number">3</span>,sep=<span class="string">"\t"</span>,names=[<span class="string">'stopword'</span>], encoding=<span class="string">'utf-8'</span>)<span class="comment">#quoting=3全不引用</span></span><br><span class="line">    words_df=words_df[~words_df.segment.isin(stopwords.stopword)]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#统计词频</span></span><br><span class="line">    words_stat=words_df.groupby(by=[<span class="string">'segment'</span>])[<span class="string">'segment'</span>].agg(&#123;<span class="string">"计数"</span>:numpy.size&#125;)</span><br><span class="line">    words_stat=words_stat.reset_index().sort_values(by=[<span class="string">"计数"</span>],ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#用词云进行显示</span></span><br><span class="line">    wordcloud=WordCloud(font_path=<span class="string">"simhei.ttf"</span>,background_color=<span class="string">"white"</span>,max_font_size=<span class="number">80</span>)</span><br><span class="line">    word_frequence = &#123;x[<span class="number">0</span>]:x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> words_stat.head(<span class="number">1000</span>).values&#125;</span><br><span class="line"></span><br><span class="line">    word_frequence_list = []</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> word_frequence:</span><br><span class="line">        temp = (key,word_frequence[key])</span><br><span class="line">        word_frequence_list.append(temp)</span><br><span class="line"></span><br><span class="line">    wordcloud=wordcloud.fit_words(word_frequence_list)</span><br><span class="line">    plt.imshow(wordcloud)</span><br><span class="line"></span><br><span class="line"><span class="comment">#主函数</span></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>转载自 链接地址: <a href="http://python.jobbole.com/88325/" target="_blank" rel="noopener">http://python.jobbole.com/88325/</a></p>
</blockquote>
<blockquote>
<p>个人博客 欢迎来访： <a href="http://zj2626.com" target="_blank" rel="noopener">http://zj2626.com</a></p>
</blockquote>
</div><script type="text/javascript" src="/js/share.js?v=0.0.1" async></script><a class="article-share-link" data-url="http://zj2626.github.io/2020/01/15/20171214_crawler/" data-id="ckpe2l00o00a310ujh32f5tji" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACNElEQVR42u3aQZLCMAxE0dz/0p7tVFHRdEtOGEvfKwoC4bEQtlrXJa/1sX4/H1+vfHL8zLYFAwaMYxkrXPE1+o2XsD4/TbkeBgwYcxhxKdTLol5qlfcq3w0GDBgwYkZcKOMr9ffCgAEDRo6RK5fx9u4LBRcGDBhHMdxjqn68dIvy42dxGDBgHMjQj5fvP34k34ABA8ZRjGUu/ROUjaAbA9zeCwYMGK0Z7wSQSgMuN6IBAwaMCYxdYxZuqdU3ghIMBgwYIxl72/q59pk+LgYDBoxpDKWM1gNLt+D+8SoMGDBaM9wRB307qBfcSiGGAQPGTIa7HcwVx0p4cLm/LgwYMA5n6KVW+j1SJTUXIdzucGHAgNGU4Q5A5MbIHmzkwYABozUjlwO6xbRyALY3qTBgwGjH0N/mFla9QMejGMYhFgYMGE0Zbs/KpSqbuV0BKgwYMLoy3Ka8GyG4pVNvtBnJBgwYMNoxlMOnUkD1g2tu2AIGDBhzGMrLuYEtt7xWYgYYMGBMYLjxZO6r79pQ3t4LBgwYYxi5pnxuFKwSUdwGmTBgwBjAqMSNucizvk2EAQNGb8Yylx465gYv3Pbc5d4eBgwYBzJyh9Xc2EQlNC0FnDBgwGjB0Ius3jirBJPKd7D/PWDAgNGCUT9wugMZlUPsbcGFAQMGDDmY1K95aSAMBgwYwxhuqKk/7x6hH/nfgAEDxj9m5MKAekNfb8Zta7fBgAHjQEZlUiMXWFbiyVKQCQMGjPMYP/B0GQZ6KI6jAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/python/">python</a><a href="/tags/爬虫/">爬虫</a></div><div class="post-nav"><a class="pre" href="/2020/01/15/20171219_crawler2/">Python 爬虫实战（2）</a><a class="next" href="/2020/01/15/20170924_VM-options配置/">VM options配置</a></div><div id="container"></div><link rel="stylesheet" href="/css/default.css?v=0.0.1"><script src="/js/gitment.browser.js?v=0.0.1"></script><script>var gitment = new Gitment({
  owner: 'zj2626',
  repo: 'zj2626.github.io',
  oauth: {
    client_id: '22769c7edffa5f05d10d',
    client_secret: '7bac8fc03397cb64c178fbdfe3a01d2abb459704',
  },
})
gitment.render('container')
</script></div></div></div><div class="pure-u-1 pure-u-md-1-4"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/BUG解决/">BUG解决</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DOM操作/">DOM操作</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/DOM操作/XML/">XML</a><span class="category-list-count">6</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Git/">Git</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/JDBC/">JDBC</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java虚拟机/">java虚拟机</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/java语言基础/">java语言基础</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/多线程/">多线程</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库连接池/">数据库连接池</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据结构和算法/">数据结构和算法</a><span class="category-list-count">16</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/框架相关/">框架相关</a><span class="category-list-count">6</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/框架相关/前端技术/">前端技术</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/框架相关/权限管理/">权限管理</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/正则/">正则</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/爬虫/">爬虫</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/程序安装与配置/">程序安装与配置</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/计算机网络/">计算机网络</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/存储过程/" style="font-size: 15px;">存储过程</a> <a href="/tags/深入了解java虚拟机/" style="font-size: 15px;">深入了解java虚拟机</a> <a href="/tags/Maven/" style="font-size: 15px;">Maven</a> <a href="/tags/jenkins/" style="font-size: 15px;">jenkins</a> <a href="/tags/Git/" style="font-size: 15px;">Git</a> <a href="/tags/maven/" style="font-size: 15px;">maven</a> <a href="/tags/HTML/" style="font-size: 15px;">HTML</a> <a href="/tags/iframe/" style="font-size: 15px;">iframe</a> <a href="/tags/Hexo优化/" style="font-size: 15px;">Hexo优化</a> <a href="/tags/C语言/" style="font-size: 15px;">C语言</a> <a href="/tags/junit/" style="font-size: 15px;">junit</a> <a href="/tags/定义/" style="font-size: 15px;">定义</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/转码/" style="font-size: 15px;">转码</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/常用命令/" style="font-size: 15px;">常用命令</a> <a href="/tags/shell/" style="font-size: 15px;">shell</a> <a href="/tags/爬虫/" style="font-size: 15px;">爬虫</a> <a href="/tags/Dom4j/" style="font-size: 15px;">Dom4j</a> <a href="/tags/Mongodb/" style="font-size: 15px;">Mongodb</a> <a href="/tags/mysql/" style="font-size: 15px;">mysql</a> <a href="/tags/java/" style="font-size: 15px;">java</a> <a href="/tags/XPath/" style="font-size: 15px;">XPath</a> <a href="/tags/C3P0/" style="font-size: 15px;">C3P0</a> <a href="/tags/Vue/" style="font-size: 15px;">Vue</a> <a href="/tags/DBCP/" style="font-size: 15px;">DBCP</a> <a href="/tags/DRUID/" style="font-size: 15px;">DRUID</a> <a href="/tags/工具类/" style="font-size: 15px;">工具类</a> <a href="/tags/jaxp/" style="font-size: 15px;">jaxp</a> <a href="/tags/Blob/" style="font-size: 15px;">Blob</a> <a href="/tags/CRUD/" style="font-size: 15px;">CRUD</a> <a href="/tags/事务/" style="font-size: 15px;">事务</a> <a href="/tags/sax/" style="font-size: 15px;">sax</a> <a href="/tags/JDBC/" style="font-size: 15px;">JDBC</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/注解/" style="font-size: 15px;">注解</a> <a href="/tags/元数据/" style="font-size: 15px;">元数据</a> <a href="/tags/Hibernate/" style="font-size: 15px;">Hibernate</a> <a href="/tags/Mybatis/" style="font-size: 15px;">Mybatis</a> <a href="/tags/ThreadLocal/" style="font-size: 15px;">ThreadLocal</a> <a href="/tags/逻辑回归/" style="font-size: 15px;">逻辑回归</a> <a href="/tags/Shiro/" style="font-size: 15px;">Shiro</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2021/05/31/20210601001_OutOfMemoryError/">4.OutOfMemoryError</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/2018031301/">使用多种算法对泰坦尼克号乘客获救原因进行分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/2018031202/">关于使用sklearn进行数据预处理-归一化/标准化/正则化(转)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/2018031201/">scikit-learn数据预处理fit_transform()与transform()的区别(转)</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/2018030701/">构建逻辑回归模型实例</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/20171225_regular/">正则表达式</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/20171222_crawler4/">Python 爬虫实战（4）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/20171219_python_error/">TypeError, a bytes-like object is required, not 'str'</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/20171219_crawler3/">Python 爬虫实战（3）</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/15/20171219_crawler2/">Python 爬虫实战（2）</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://github.com/zj2626/" title="github" target="_blank">github</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2021 <a href="/." rel="nofollow">ZJ_BLOG.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.1" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.1" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.1"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
    search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>var _hmt = _hmt || [];
(function () {
    var hm = document.createElement("script");
    hm.src = '//hm.baidu.com/hm.js?' + 'c9a692191e9aca9e30daa3f6326cc789';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(hm, s);
})();
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.1"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.1"></script></div></body></html>